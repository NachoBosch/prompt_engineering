{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# M√≥dulo 3: Context Engineering\n",
        "## Uso de Herramientas y Fuentes Externas para Mejorar Capacidades de IA\n",
        "\n",
        "**Duraci√≥n estimada**: 1 hora  \n",
        "**Nivel**: Avanzado  \n",
        "**Objetivo**: Implementar context engineering con herramientas externas y RAG\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Contenido del M√≥dulo\n",
        "\n",
        "Este notebook cubre:\n",
        "1. **Pr√°ctica 3.1**: RAG b√°sico con documentos\n",
        "2. **Pr√°ctica 3.2**: RAG avanzado con m√∫ltiples fuentes\n",
        "3. **Pr√°ctica 3.3**: Sistema completo de context engineering\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Objetivos de Aprendizaje\n",
        "\n",
        "Al finalizar este m√≥dulo ser√°s capaz de:\n",
        "- Implementar sistemas RAG (Retrieval-Augmented Generation) b√°sicos\n",
        "- Usar embeddings y b√∫squeda sem√°ntica para encontrar informaci√≥n relevante\n",
        "- Integrar m√∫ltiples fuentes de contexto (documentos, bases de datos, APIs)\n",
        "- Construir sistemas completos de context engineering para producci√≥n\n",
        "- Optimizar sistemas RAG para mejor rendimiento\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Configuraci√≥n Inicial\n",
        "\n",
        "Antes de comenzar, aseg√∫rate de tener:\n",
        "- Python 3.8+ instalado\n",
        "- Cuenta de OpenAI con API key\n",
        "- Librer√≠as necesarias instaladas\n",
        "\n",
        "**Instalaci√≥n en Windows:**\n",
        "```bash\n",
        "pip install openai python-dotenv sentence-transformers chromadb langchain\n",
        "```\n",
        "\n",
        "**Nota**: La instalaci√≥n de `sentence-transformers` puede tardar varios minutos la primera vez."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalaci√≥n de dependencias (ejecutar solo si es necesario)\n",
        "# !pip install openai python-dotenv sentence-transformers chromadb langchain\n",
        "\n",
        "# Importar librer√≠as necesarias\n",
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import hashlib\n",
        "\n",
        "# Cargar variables de entorno\n",
        "load_dotenv()\n",
        "\n",
        "# Inicializar cliente de OpenAI\n",
        "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "\n",
        "# Verificar configuraci√≥n\n",
        "if not client.api_key:\n",
        "    print(\"‚ö†Ô∏è ADVERTENCIA: No se encontr√≥ OPENAI_API_KEY. Config√∫rala en un archivo .env\")\n",
        "else:\n",
        "    print(\"‚úÖ Cliente de OpenAI configurado correctamente\")\n",
        "\n",
        "# Intentar importar librer√≠as opcionales\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import chromadb\n",
        "    from chromadb.config import Settings\n",
        "    EMBEDDINGS_AVAILABLE = True\n",
        "    print(\"‚úÖ Librer√≠as de embeddings disponibles\")\n",
        "except ImportError:\n",
        "    EMBEDDINGS_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è Algunas librer√≠as no est√°n disponibles. Algunas funciones pueden no funcionar.\")\n",
        "    print(\"   Instala con: pip install sentence-transformers chromadb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Pr√°ctica 3.1: RAG B√°sico con Documentos\n",
        "\n",
        "## üéØ Objetivo\n",
        "Implementar un sistema RAG (Retrieval-Augmented Generation) simple que permita hacer preguntas sobre documentos usando embeddings y b√∫squeda sem√°ntica.\n",
        "\n",
        "## üìñ Contexto y Teor√≠a\n",
        "\n",
        "### ¬øQu√© es RAG?\n",
        "\n",
        "**RAG (Retrieval-Augmented Generation)** es una t√©cnica que:\n",
        "1. **Retrieval (Recuperaci√≥n)**: Busca informaci√≥n relevante en una base de conocimiento\n",
        "2. **Augmentation (Aumento)**: A√±ade esa informaci√≥n al prompt\n",
        "3. **Generation (Generaci√≥n)**: El modelo genera respuestas usando el contexto recuperado\n",
        "\n",
        "### ¬øPor qu√© usar RAG?\n",
        "\n",
        "- ‚úÖ **Informaci√≥n actualizada**: Acceso a datos que el modelo no tiene en su entrenamiento\n",
        "- ‚úÖ **Fuentes verificables**: Puedes citar de d√≥nde viene la informaci√≥n\n",
        "- ‚úÖ **Menor alucinaci√≥n**: El modelo tiene contexto real para responder\n",
        "- ‚úÖ **Especializaci√≥n**: Puedes usar conocimiento espec√≠fico del dominio\n",
        "\n",
        "### Arquitectura RAG B√°sica\n",
        "\n",
        "```\n",
        "1. Documentos ‚Üí Chunking (dividir en fragmentos)\n",
        "2. Chunks ‚Üí Embeddings (convertir a vectores)\n",
        "3. Almacenar en Vector Store\n",
        "4. Query ‚Üí Embedding de la pregunta\n",
        "5. B√∫squeda sem√°ntica ‚Üí Encontrar chunks relevantes\n",
        "6. Chunks + Pregunta ‚Üí Prompt al modelo\n",
        "7. Respuesta generada\n",
        "```\n",
        "\n",
        "### Casos de Uso\n",
        "- Q&A sobre documentaci√≥n t√©cnica\n",
        "- Asistentes con conocimiento corporativo\n",
        "- B√∫squeda en c√≥digo legacy\n",
        "- Sistemas de ayuda contextual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Ejercicio Guiado: Sistema RAG B√°sico\n",
        "\n",
        "Vamos a crear un sistema RAG simple paso a paso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EJEMPLO: Sistema RAG b√°sico usando OpenAI Embeddings\n",
        "\n",
        "# Documentos de ejemplo (en producci√≥n vendr√≠an de archivos, bases de datos, etc.)\n",
        "documentos = [\n",
        "    \"Python es un lenguaje de programaci√≥n de alto nivel, interpretado y de prop√≥sito general. Fue creado por Guido van Rossum y se lanz√≥ por primera vez en 1991.\",\n",
        "    \"OpenAI es una empresa de investigaci√≥n en inteligencia artificial fundada en 2015. Es conocida por desarrollar modelos como GPT-3, GPT-4 y DALL-E.\",\n",
        "    \"Los embeddings son representaciones vectoriales de texto que capturan el significado sem√°ntico. Permiten comparar textos por similitud en lugar de coincidencia exacta.\",\n",
        "    \"RAG (Retrieval-Augmented Generation) combina b√∫squeda de informaci√≥n con generaci√≥n de texto. Primero busca informaci√≥n relevante, luego la usa para generar respuestas.\",\n",
        "    \"Jupyter Notebook es un entorno interactivo que permite combinar c√≥digo, texto y visualizaciones. Es muy popular en ciencia de datos y an√°lisis exploratorio.\"\n",
        "]\n",
        "\n",
        "def crear_embeddings(textos: List[str]) -> List[List[float]]:\n",
        "    \"\"\"\n",
        "    Crea embeddings usando OpenAI\n",
        "    \"\"\"\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",  # Modelo econ√≥mico y eficiente\n",
        "        input=textos\n",
        "    )\n",
        "    return [item.embedding for item in response.data]\n",
        "\n",
        "def calcular_similitud_coseno(vec1: List[float], vec2: List[float]) -> float:\n",
        "    \"\"\"\n",
        "    Calcula similitud coseno entre dos vectores\n",
        "    \"\"\"\n",
        "    import math\n",
        "    \n",
        "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
        "    magnitude1 = math.sqrt(sum(a * a for a in vec1))\n",
        "    magnitude2 = math.sqrt(sum(a * a for a in vec2))\n",
        "    \n",
        "    if magnitude1 == 0 or magnitude2 == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    return dot_product / (magnitude1 * magnitude2)\n",
        "\n",
        "def buscar_documentos_relevantes(pregunta: str, documentos: List[str], top_k: int = 2) -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Busca los documentos m√°s relevantes para una pregunta\n",
        "    \"\"\"\n",
        "    # Crear embedding de la pregunta\n",
        "    embedding_pregunta = crear_embeddings([pregunta])[0]\n",
        "    \n",
        "    # Crear embeddings de todos los documentos\n",
        "    embeddings_docs = crear_embeddings(documentos)\n",
        "    \n",
        "    # Calcular similitud con cada documento\n",
        "    similitudes = []\n",
        "    for doc, emb in zip(documentos, embeddings_docs):\n",
        "        similitud = calcular_similitud_coseno(embedding_pregunta, emb)\n",
        "        similitudes.append((doc, similitud))\n",
        "    \n",
        "    # Ordenar por similitud y retornar top_k\n",
        "    similitudes.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similitudes[:top_k]\n",
        "\n",
        "def sistema_rag_basico(pregunta: str, documentos: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Sistema RAG b√°sico completo\n",
        "    \"\"\"\n",
        "    print(f\"‚ùì Pregunta: {pregunta}\\n\")\n",
        "    \n",
        "    # Paso 1: Buscar documentos relevantes\n",
        "    print(\"üîç Buscando documentos relevantes...\")\n",
        "    documentos_relevantes = buscar_documentos_relevantes(pregunta, documentos, top_k=2)\n",
        "    \n",
        "    print(f\"‚úÖ Encontrados {len(documentos_relevantes)} documentos relevantes:\")\n",
        "    for i, (doc, sim) in enumerate(documentos_relevantes, 1):\n",
        "        print(f\"   {i}. Similitud: {sim:.3f} - {doc[:60]}...\")\n",
        "    \n",
        "    # Paso 2: Construir contexto\n",
        "    contexto = \"\\n\\n\".join([doc for doc, _ in documentos_relevantes])\n",
        "    \n",
        "    # Paso 3: Generar respuesta usando el contexto\n",
        "    print(\"\\nü§ñ Generando respuesta...\")\n",
        "    prompt = f\"\"\"\n",
        "Bas√°ndote en la siguiente informaci√≥n, responde la pregunta.\n",
        "Si la informaci√≥n no es suficiente, di que no tienes suficiente informaci√≥n.\n",
        "\n",
        "Informaci√≥n:\n",
        "{contexto}\n",
        "\n",
        "Pregunta: {pregunta}\n",
        "\n",
        "Responde de manera clara y concisa:\n",
        "\"\"\"\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3\n",
        "    )\n",
        "    \n",
        "    respuesta = response.choices[0].message.content\n",
        "    return respuesta\n",
        "\n",
        "# Probar el sistema RAG\n",
        "print(\"=\" * 60)\n",
        "print(\"SISTEMA RAG B√ÅSICO\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "preguntas = [\n",
        "    \"¬øQu√© es Python?\",\n",
        "    \"¬øQu√© es RAG y c√≥mo funciona?\",\n",
        "    \"¬øQui√©n cre√≥ Python y cu√°ndo?\"\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    respuesta = sistema_rag_basico(pregunta, documentos)\n",
        "    print(f\"\\nüí¨ Respuesta: {respuesta}\\n\")\n",
        "    print(\"-\" * 60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ESPACIO PARA TU C√ìDIGO\n",
        "def chunk_documento(documento: str, tamano_chunk: int = 200) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Divide un documento en chunks de tama√±o aproximado\n",
        "    Retorna lista de chunks con metadatos\n",
        "    \"\"\"\n",
        "    # TODO: Implementa el chunking\n",
        "    # - Divide el documento en palabras\n",
        "    # - Agrupa en chunks de ~tamano_chunk palabras\n",
        "    # - Incluye metadatos (√≠ndice, documento original)\n",
        "    pass\n",
        "\n",
        "def sistema_rag_mejorado(pregunta: str, documentos: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Sistema RAG mejorado con chunking\n",
        "    \"\"\"\n",
        "    # TODO: Implementa el sistema mejorado\n",
        "    # 1. Chunkear todos los documentos\n",
        "    # 2. Crear embeddings de los chunks\n",
        "    # 3. Buscar chunks relevantes\n",
        "    # 4. Generar respuesta con referencias\n",
        "    pass\n",
        "\n",
        "# Documentos de prueba (m√°s largos)\n",
        "documentos_largos = [\n",
        "    \"\"\"\n",
        "    Python es un lenguaje de programaci√≥n de alto nivel, interpretado y de prop√≥sito general. \n",
        "    Fue creado por Guido van Rossum y se lanz√≥ por primera vez en 1991. Python tiene una filosof√≠a \n",
        "    de dise√±o que enfatiza la legibilidad del c√≥digo, notablemente mediante el uso de espacios en \n",
        "    blanco significativos. Sus construcciones de lenguaje y su enfoque orientado a objetos tienen \n",
        "    como objetivo ayudar a los programadores a escribir c√≥digo claro y l√≥gico para proyectos \n",
        "    peque√±os y grandes. Python es din√°micamente tipado y recolecta basura. Soporta m√∫ltiples \n",
        "    paradigmas de programaci√≥n, incluyendo programaci√≥n estructurada, orientada a objetos y funcional.\n",
        "    \"\"\",\n",
        "    # Agrega m√°s documentos largos aqu√≠\n",
        "]\n",
        "\n",
        "# Prueba el sistema\n",
        "# respuesta = sistema_rag_mejorado(\"¬øCu√°les son las caracter√≠sticas principales de Python?\", documentos_largos)\n",
        "# print(respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî• Ejercicio de Desaf√≠o 3.1 (Opcional)\n",
        "\n",
        "Crea un sistema RAG que:\n",
        "- Use ChromaDB o FAISS para almacenamiento vectorial persistente\n",
        "- Implemente re-ranking de resultados (usar m√∫ltiples estrategias de b√∫squeda)\n",
        "- Incluya filtrado por metadatos (fecha, categor√≠a, etc.)\n",
        "- Genere respuestas con citas numeradas a los documentos fuente\n",
        "\n",
        "**Desaf√≠o extra**: Implementa un sistema de actualizaci√≥n incremental (agregar nuevos documentos sin re-indexar todo)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Pr√°ctica 3.2: RAG Avanzado con M√∫ltiples Fuentes\n",
        "\n",
        "## üéØ Objetivo\n",
        "Integrar m√∫ltiples fuentes de contexto (documentos, bases de datos, APIs) en un sistema RAG avanzado.\n",
        "\n",
        "## üìñ Contexto y Teor√≠a\n",
        "\n",
        "### ¬øPor qu√© M√∫ltiples Fuentes?\n",
        "\n",
        "En sistemas reales, la informaci√≥n puede venir de:\n",
        "- **Documentos**: PDFs, Markdown, texto plano\n",
        "- **Bases de datos**: SQL, NoSQL, datos estructurados\n",
        "- **APIs**: Servicios externos, datos en tiempo real\n",
        "- **C√≥digo**: Repositorios, documentaci√≥n t√©cnica\n",
        "\n",
        "### Arquitectura Multi-Fuente\n",
        "\n",
        "```\n",
        "Fuente 1 (Docs) ‚Üí Embeddings ‚Üí Vector Store\n",
        "Fuente 2 (DB)   ‚Üí Query SQL   ‚Üí Resultados\n",
        "Fuente 3 (API)  ‚Üí HTTP Request ‚Üí Datos JSON\n",
        "                    ‚Üì\n",
        "            Agregador de Contexto\n",
        "                    ‚Üì\n",
        "            Prompt con Todo el Contexto\n",
        "                    ‚Üì\n",
        "            Respuesta Final\n",
        "```\n",
        "\n",
        "### Estrategias de Agregaci√≥n\n",
        "\n",
        "1. **Fusi√≥n temprana**: Combinar antes de buscar\n",
        "2. **Fusi√≥n tard√≠a**: Buscar en cada fuente, luego combinar resultados\n",
        "3. **H√≠brida**: Combinar b√∫squeda sem√°ntica + b√∫squeda estructurada\n",
        "\n",
        "### Casos de Uso\n",
        "- Sistemas de consulta corporativa\n",
        "- Asistentes con acceso a m√∫ltiples sistemas\n",
        "- An√°lisis de datos multi-fuente\n",
        "- Sistemas de recomendaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Ejercicio Guiado: Sistema RAG Multi-Fuente\n",
        "\n",
        "Vamos a crear un sistema que combine documentos, base de datos simulada y API simulada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EJEMPLO: Sistema RAG con m√∫ltiples fuentes\n",
        "\n",
        "# Fuente 1: Documentos (ya implementado)\n",
        "documentos = [\n",
        "    \"Python es un lenguaje de programaci√≥n vers√°til usado en ciencia de datos, web development y automatizaci√≥n.\",\n",
        "    \"OpenAI proporciona APIs para integraci√≥n de IA en aplicaciones mediante modelos como GPT-4.\",\n",
        "    \"Los embeddings permiten b√∫squeda sem√°ntica encontrando textos similares en significado.\"\n",
        "]\n",
        "\n",
        "# Fuente 2: Base de datos simulada\n",
        "base_datos_usuarios = {\n",
        "    \"usuarios\": [\n",
        "        {\"id\": 1, \"nombre\": \"Juan\", \"rol\": \"desarrollador\", \"proyecto\": \"API Backend\"},\n",
        "        {\"id\": 2, \"nombre\": \"Mar√≠a\", \"rol\": \"data scientist\", \"proyecto\": \"ML Models\"},\n",
        "        {\"id\": 3, \"nombre\": \"Carlos\", \"rol\": \"devops\", \"proyecto\": \"Infrastructure\"}\n",
        "    ],\n",
        "    \"proyectos\": [\n",
        "        {\"id\": 1, \"nombre\": \"API Backend\", \"tecnologia\": \"Python, FastAPI\", \"estado\": \"activo\"},\n",
        "        {\"id\": 2, \"nombre\": \"ML Models\", \"tecnologia\": \"Python, TensorFlow\", \"estado\": \"activo\"},\n",
        "        {\"id\": 3, \"nombre\": \"Infrastructure\", \"tecnologia\": \"Docker, Kubernetes\", \"estado\": \"planning\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "def consultar_base_datos(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Simula consulta a base de datos basada en palabras clave\n",
        "    \"\"\"\n",
        "    query_lower = query.lower()\n",
        "    resultados = []\n",
        "    \n",
        "    # B√∫squeda simple por palabras clave\n",
        "    if \"usuario\" in query_lower or \"persona\" in query_lower:\n",
        "        for usuario in base_datos_usuarios[\"usuarios\"]:\n",
        "            if any(palabra in str(usuario).lower() for palabra in query_lower.split()):\n",
        "                resultados.append(f\"Usuario: {usuario['nombre']}, Rol: {usuario['rol']}, Proyecto: {usuario['proyecto']}\")\n",
        "    \n",
        "    if \"proyecto\" in query_lower:\n",
        "        for proyecto in base_datos_usuarios[\"proyectos\"]:\n",
        "            if any(palabra in str(proyecto).lower() for palabra in query_lower.split()):\n",
        "                resultados.append(f\"Proyecto: {proyecto['nombre']}, Tecnolog√≠a: {proyecto['tecnologia']}, Estado: {proyecto['estado']}\")\n",
        "    \n",
        "    return \"\\n\".join(resultados) if resultados else \"No se encontraron resultados en la base de datos.\"\n",
        "\n",
        "# Fuente 3: API simulada\n",
        "def consultar_api(tema: str) -> str:\n",
        "    \"\"\"\n",
        "    Simula consulta a API externa\n",
        "    \"\"\"\n",
        "    # Simulaci√≥n de datos de API\n",
        "    datos_api = {\n",
        "        \"python\": \"Python 3.11 es la versi√≥n m√°s reciente con mejoras en rendimiento y nuevas caracter√≠sticas.\",\n",
        "        \"openai\": \"OpenAI API est√° disponible 24/7 con rate limits seg√∫n el plan de suscripci√≥n.\",\n",
        "        \"rag\": \"RAG es una t√©cnica que combina recuperaci√≥n de informaci√≥n con generaci√≥n de texto.\"\n",
        "    }\n",
        "    \n",
        "    for clave, valor in datos_api.items():\n",
        "        if clave in tema.lower():\n",
        "            return f\"Datos de API: {valor}\"\n",
        "    \n",
        "    return \"No hay informaci√≥n disponible en la API para este tema.\"\n",
        "\n",
        "def sistema_rag_multifuente(pregunta: str) -> str:\n",
        "    \"\"\"\n",
        "    Sistema RAG que combina m√∫ltiples fuentes\n",
        "    \"\"\"\n",
        "    print(f\"‚ùì Pregunta: {pregunta}\\n\")\n",
        "    \n",
        "    contexto_completo = []\n",
        "    fuentes_usadas = []\n",
        "    \n",
        "    # Fuente 1: B√∫squeda en documentos (RAG sem√°ntico)\n",
        "    print(\"üîç Buscando en documentos...\")\n",
        "    documentos_relevantes = buscar_documentos_relevantes(pregunta, documentos, top_k=2)\n",
        "    if documentos_relevantes:\n",
        "        contexto_docs = \"\\n\".join([doc for doc, _ in documentos_relevantes])\n",
        "        contexto_completo.append(f\"DOCUMENTOS:\\n{contexto_docs}\")\n",
        "        fuentes_usadas.append(\"documentos\")\n",
        "        print(f\"‚úÖ Encontrados {len(documentos_relevantes)} documentos relevantes\")\n",
        "    \n",
        "    # Fuente 2: Consulta a base de datos\n",
        "    print(\"\\nüíæ Consultando base de datos...\")\n",
        "    resultado_db = consultar_base_datos(pregunta)\n",
        "    if resultado_db and \"No se encontraron\" not in resultado_db:\n",
        "        contexto_completo.append(f\"BASE DE DATOS:\\n{resultado_db}\")\n",
        "        fuentes_usadas.append(\"base de datos\")\n",
        "        print(\"‚úÖ Datos encontrados en base de datos\")\n",
        "    \n",
        "    # Fuente 3: Consulta a API\n",
        "    print(\"\\nüåê Consultando API externa...\")\n",
        "    resultado_api = consultar_api(pregunta)\n",
        "    if resultado_api and \"No hay informaci√≥n\" not in resultado_api:\n",
        "        contexto_completo.append(f\"API EXTERNA:\\n{resultado_api}\")\n",
        "        fuentes_usadas.append(\"API\")\n",
        "        print(\"‚úÖ Datos obtenidos de API\")\n",
        "    \n",
        "    # Combinar todo el contexto\n",
        "    contexto_final = \"\\n\\n\".join(contexto_completo)\n",
        "    \n",
        "    if not contexto_final:\n",
        "        return \"No se encontr√≥ informaci√≥n relevante en ninguna fuente disponible.\"\n",
        "    \n",
        "    # Generar respuesta\n",
        "    print(f\"\\nü§ñ Generando respuesta usando {len(fuentes_usadas)} fuente(s)...\")\n",
        "    prompt = f\"\"\"\n",
        "Bas√°ndote en la siguiente informaci√≥n de m√∫ltiples fuentes, responde la pregunta de manera completa.\n",
        "\n",
        "Informaci√≥n disponible:\n",
        "{contexto_final}\n",
        "\n",
        "Pregunta: {pregunta}\n",
        "\n",
        "Responde de manera clara, citando de qu√© fuente viene cada parte de la informaci√≥n cuando sea relevante.\n",
        "\"\"\"\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Probar el sistema multi-fuente\n",
        "print(\"=\" * 60)\n",
        "print(\"SISTEMA RAG MULTI-FUENTE\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "preguntas = [\n",
        "    \"¬øQu√© es Python y qui√©n lo usa en la empresa?\",\n",
        "    \"¬øQu√© proyectos est√°n activos y qu√© tecnolog√≠as usan?\",\n",
        "    \"¬øC√≥mo funciona RAG seg√∫n la documentaci√≥n y la API?\"\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    respuesta = sistema_rag_multifuente(pregunta)\n",
        "    print(f\"\\nüí¨ Respuesta:\\n{respuesta}\\n\")\n",
        "    print(\"=\" * 60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ESPACIO PARA TU C√ìDIGO\n",
        "\n",
        "# C√≥digo fuente simulado\n",
        "codigo_fuente = {\n",
        "    \"calculadora.py\": \"\"\"\n",
        "def suma(a, b):\n",
        "    return a + b\n",
        "\n",
        "def multiplicar(a, b):\n",
        "    return a * b\n",
        "\"\"\",\n",
        "    \"utils.py\": \"\"\"\n",
        "def procesar_datos(datos):\n",
        "    resultados = []\n",
        "    for item in datos:\n",
        "        resultados.append(item * 2)\n",
        "    return resultados\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "# TODO: Implementa las funciones de b√∫squeda\n",
        "def buscar_en_codigo(pregunta: str) -> str:\n",
        "    \"\"\"Busca funciones/clases relevantes en el c√≥digo\"\"\"\n",
        "    # Tu implementaci√≥n aqu√≠\n",
        "    pass\n",
        "\n",
        "def calcular_metricas_codigo(archivo: str, codigo: str) -> Dict[str, Any]:\n",
        "    \"\"\"Calcula m√©tricas de c√≥digo\"\"\"\n",
        "    # Tu implementaci√≥n aqu√≠\n",
        "    pass\n",
        "\n",
        "def sistema_rag_codigo_docs(pregunta: str) -> str:\n",
        "    \"\"\"\n",
        "    Sistema RAG que integra c√≥digo, documentaci√≥n y m√©tricas\n",
        "    \"\"\"\n",
        "    # TODO: Implementa el sistema completo\n",
        "    # 1. Analizar la pregunta para determinar qu√© fuentes usar\n",
        "    # 2. Buscar en cada fuente relevante\n",
        "    # 3. Agregar resultados\n",
        "    # 4. Generar respuesta consolidada\n",
        "    pass\n",
        "\n",
        "# Prueba el sistema\n",
        "# preguntas_test = [\n",
        "#     \"¬øC√≥mo funciona la funci√≥n suma?\",\n",
        "#     \"¬øCu√°l es la complejidad del c√≥digo en calculadora.py?\",\n",
        "#     \"¬øQu√© funciones hay disponibles para procesar datos?\"\n",
        "# ]\n",
        "# \n",
        "# for pregunta in preguntas_test:\n",
        "#     print(sistema_rag_codigo_docs(pregunta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî• Ejercicio de Desaf√≠o 3.2 (Opcional)\n",
        "\n",
        "Crea un sistema RAG que:\n",
        "- Use un router inteligente que decida qu√© fuente consultar primero\n",
        "- Implemente b√∫squeda h√≠brida (sem√°ntica + keyword)\n",
        "- Incluya sistema de scoring para rankear resultados de m√∫ltiples fuentes\n",
        "- Genere respuestas con confianza score basado en calidad de las fuentes\n",
        "\n",
        "**Desaf√≠o extra**: Implementa un sistema de feedback que aprenda qu√© fuentes son m√°s √∫tiles para cada tipo de pregunta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Pr√°ctica 3.3: Sistema Completo de Context Engineering\n",
        "\n",
        "## üéØ Objetivo\n",
        "Construir una soluci√≥n end-to-end de context engineering con RAG, validaci√≥n, optimizaci√≥n y caracter√≠sticas de producci√≥n.\n",
        "\n",
        "## üìñ Contexto y Teor√≠a\n",
        "\n",
        "### Componentes de un Sistema Completo\n",
        "\n",
        "1. **Ingesti√≥n de datos**: Cargar y procesar m√∫ltiples tipos de documentos\n",
        "2. **Chunking inteligente**: Dividir documentos preservando contexto\n",
        "3. **Embeddings y almacenamiento**: Vector store persistente\n",
        "4. **B√∫squeda avanzada**: M√∫ltiples estrategias de retrieval\n",
        "5. **Generaci√≥n con contexto**: Prompt engineering optimizado\n",
        "6. **Validaci√≥n y calidad**: Verificar respuestas\n",
        "7. **Monitoreo**: Tracking de uso y performance\n",
        "\n",
        "### Optimizaciones Importantes\n",
        "\n",
        "- **Chunking estrat√©gico**: Tama√±o √≥ptimo seg√∫n tipo de documento\n",
        "- **Re-ranking**: Mejorar resultados de b√∫squeda inicial\n",
        "- **Cach√©**: Evitar rec√°lculos costosos\n",
        "- **Filtrado**: Usar metadatos para refinar b√∫squedas\n",
        "- **Hybrid search**: Combinar b√∫squeda sem√°ntica y keyword\n",
        "\n",
        "### Casos de Uso\n",
        "- Asistentes corporativos con conocimiento interno\n",
        "- Sistemas de Q&A sobre documentaci√≥n t√©cnica\n",
        "- Agentes con memoria persistente\n",
        "- Sistemas de recomendaci√≥n basados en contenido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Ejercicio Guiado: Sistema Completo de Context Engineering\n",
        "\n",
        "Vamos a construir un sistema completo con todas las caracter√≠sticas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EJEMPLO: Sistema completo de context engineering\n",
        "\n",
        "class SistemaContextEngineering:\n",
        "    \"\"\"\n",
        "    Sistema completo de context engineering con todas las caracter√≠sticas\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.documentos = []\n",
        "        self.embeddings_cache = {}  # Cach√© de embeddings\n",
        "        self.queries_cache = {}     # Cach√© de queries\n",
        "        \n",
        "    def agregar_documento(self, documento: str, metadatos: Dict = None):\n",
        "        \"\"\"Agrega un documento al sistema\"\"\"\n",
        "        doc_id = hashlib.md5(documento.encode()).hexdigest()[:8]\n",
        "        self.documentos.append({\n",
        "            \"id\": doc_id,\n",
        "            \"texto\": documento,\n",
        "            \"metadatos\": metadatos or {}\n",
        "        })\n",
        "        return doc_id\n",
        "    \n",
        "    def chunk_inteligente(self, texto: str, tamano: int = 200, overlap: int = 50) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Chunking inteligente con overlap para preservar contexto\n",
        "        \"\"\"\n",
        "        palabras = texto.split()\n",
        "        chunks = []\n",
        "        \n",
        "        for i in range(0, len(palabras), tamano - overlap):\n",
        "            chunk_palabras = palabras[i:i + tamano]\n",
        "            chunk_texto = \" \".join(chunk_palabras)\n",
        "            \n",
        "            chunks.append({\n",
        "                \"texto\": chunk_texto,\n",
        "                \"inicio\": i,\n",
        "                \"fin\": min(i + tamano, len(palabras))\n",
        "            })\n",
        "        \n",
        "        return chunks\n",
        "    \n",
        "    def obtener_embedding(self, texto: str, usar_cache: bool = True) -> List[float]:\n",
        "        \"\"\"Obtiene embedding con cach√©\"\"\"\n",
        "        if usar_cache:\n",
        "            texto_hash = hashlib.md5(texto.encode()).hexdigest()\n",
        "            if texto_hash in self.embeddings_cache:\n",
        "                return self.embeddings_cache[texto_hash]\n",
        "        \n",
        "        response = client.embeddings.create(\n",
        "            model=\"text-embedding-3-small\",\n",
        "            input=[texto]\n",
        "        )\n",
        "        embedding = response.data[0].embedding\n",
        "        \n",
        "        if usar_cache:\n",
        "            self.embeddings_cache[texto_hash] = embedding\n",
        "        \n",
        "        return embedding\n",
        "    \n",
        "    def buscar_relevantes(self, pregunta: str, top_k: int = 3, usar_reranking: bool = True) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        B√∫squeda avanzada con re-ranking opcional\n",
        "        \"\"\"\n",
        "        # B√∫squeda inicial\n",
        "        embedding_pregunta = self.obtener_embedding(pregunta)\n",
        "        resultados = []\n",
        "        \n",
        "        for doc in self.documentos:\n",
        "            # Chunkear si es necesario\n",
        "            if len(doc[\"texto\"].split()) > 300:\n",
        "                chunks = self.chunk_inteligente(doc[\"texto\"])\n",
        "                for chunk in chunks:\n",
        "                    embedding_chunk = self.obtener_embedding(chunk[\"texto\"])\n",
        "                    similitud = calcular_similitud_coseno(embedding_pregunta, embedding_chunk)\n",
        "                    resultados.append({\n",
        "                        \"texto\": chunk[\"texto\"],\n",
        "                        \"similitud\": similitud,\n",
        "                        \"metadatos\": doc[\"metadatos\"],\n",
        "                        \"doc_id\": doc[\"id\"]\n",
        "                    })\n",
        "            else:\n",
        "                embedding_doc = self.obtener_embedding(doc[\"texto\"])\n",
        "                similitud = calcular_similitud_coseno(embedding_pregunta, embedding_doc)\n",
        "                resultados.append({\n",
        "                    \"texto\": doc[\"texto\"],\n",
        "                    \"similitud\": similitud,\n",
        "                    \"metadatos\": doc[\"metadatos\"],\n",
        "                    \"doc_id\": doc[\"id\"]\n",
        "                })\n",
        "        \n",
        "        # Ordenar por similitud\n",
        "        resultados.sort(key=lambda x: x[\"similitud\"], reverse=True)\n",
        "        \n",
        "        # Re-ranking simple: boost si hay palabras clave en com√∫n\n",
        "        if usar_reranking:\n",
        "            palabras_pregunta = set(pregunta.lower().split())\n",
        "            for resultado in resultados[:top_k * 2]:  # Considerar m√°s para re-ranking\n",
        "                palabras_texto = set(resultado[\"texto\"].lower().split())\n",
        "                palabras_comunes = len(palabras_pregunta & palabras_texto)\n",
        "                resultado[\"similitud\"] += palabras_comunes * 0.05  # Boost peque√±o\n",
        "        \n",
        "        # Re-ordenar despu√©s de re-ranking\n",
        "        if usar_reranking:\n",
        "            resultados.sort(key=lambda x: x[\"similitud\"], reverse=True)\n",
        "        \n",
        "        return resultados[:top_k]\n",
        "    \n",
        "    def generar_respuesta(self, pregunta: str, contexto: List[Dict], validar: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Genera respuesta con validaci√≥n\n",
        "        \"\"\"\n",
        "        # Construir contexto formateado\n",
        "        contexto_texto = \"\\n\\n\".join([\n",
        "            f\"[Fuente {i+1}] {r['texto']}\" \n",
        "            for i, r in enumerate(contexto)\n",
        "        ])\n",
        "        \n",
        "        prompt = f\"\"\"\n",
        "Eres un asistente experto que responde preguntas bas√°ndote en el contexto proporcionado.\n",
        "\n",
        "Contexto:\n",
        "{contexto_texto}\n",
        "\n",
        "Pregunta: {pregunta}\n",
        "\n",
        "Instrucciones:\n",
        "- Responde de manera clara y precisa\n",
        "- Si la informaci√≥n no est√° en el contexto, di claramente que no tienes esa informaci√≥n\n",
        "- Cita las fuentes cuando sea relevante\n",
        "- S√© conciso pero completo\n",
        "\n",
        "Respuesta:\n",
        "\"\"\"\n",
        "        \n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3\n",
        "        )\n",
        "        \n",
        "        respuesta = response.choices[0].message.content\n",
        "        \n",
        "        resultado = {\n",
        "            \"respuesta\": respuesta,\n",
        "            \"fuentes\": [r[\"doc_id\"] for r in contexto],\n",
        "            \"similitudes\": [r[\"similitud\"] for r in contexto]\n",
        "        }\n",
        "        \n",
        "        # Validaci√≥n simple: verificar que la respuesta no es demasiado gen√©rica\n",
        "        if validar:\n",
        "            palabras_respuesta = set(respuesta.lower().split())\n",
        "            palabras_pregunta = set(pregunta.lower().split())\n",
        "            overlap = len(palabras_respuesta & palabras_pregunta)\n",
        "            \n",
        "            if overlap < 2 and len(respuesta) < 50:\n",
        "                resultado[\"advertencia\"] = \"La respuesta puede ser demasiado gen√©rica\"\n",
        "        \n",
        "        return resultado\n",
        "    \n",
        "    def consultar(self, pregunta: str, top_k: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        M√©todo principal para consultar el sistema\n",
        "        \"\"\"\n",
        "        print(f\"‚ùì Pregunta: {pregunta}\\n\")\n",
        "        \n",
        "        # Buscar documentos relevantes\n",
        "        print(\"üîç Buscando informaci√≥n relevante...\")\n",
        "        contexto = self.buscar_relevantes(pregunta, top_k=top_k)\n",
        "        \n",
        "        if not contexto:\n",
        "            return {\n",
        "                \"respuesta\": \"No se encontr√≥ informaci√≥n relevante para responder esta pregunta.\",\n",
        "                \"fuentes\": [],\n",
        "                \"similitudes\": []\n",
        "            }\n",
        "        \n",
        "        print(f\"‚úÖ Encontrados {len(contexto)} fragmentos relevantes\")\n",
        "        for i, r in enumerate(contexto, 1):\n",
        "            print(f\"   {i}. Similitud: {r['similitud']:.3f} - {r['texto'][:60]}...\")\n",
        "        \n",
        "        # Generar respuesta\n",
        "        print(\"\\nü§ñ Generando respuesta...\")\n",
        "        resultado = self.generar_respuesta(pregunta, contexto)\n",
        "        \n",
        "        return resultado\n",
        "\n",
        "# Crear y probar el sistema\n",
        "print(\"=\" * 60)\n",
        "print(\"SISTEMA COMPLETO DE CONTEXT ENGINEERING\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "sistema = SistemaContextEngineering()\n",
        "\n",
        "# Agregar documentos\n",
        "sistema.agregar_documento(\n",
        "    \"Python es un lenguaje de programaci√≥n de alto nivel muy popular en ciencia de datos y desarrollo web.\",\n",
        "    {\"categoria\": \"programacion\", \"fecha\": \"2024\"}\n",
        ")\n",
        "sistema.agregar_documento(\n",
        "    \"OpenAI desarrolla modelos de lenguaje como GPT-4 que pueden generar texto, c√≥digo y responder preguntas.\",\n",
        "    {\"categoria\": \"ia\", \"fecha\": \"2024\"}\n",
        ")\n",
        "sistema.agregar_documento(\n",
        "    \"RAG combina recuperaci√≥n de informaci√≥n con generaci√≥n de texto para crear sistemas de IA m√°s precisos y actualizados.\",\n",
        "    {\"categoria\": \"ia\", \"fecha\": \"2024\"}\n",
        ")\n",
        "\n",
        "# Consultas de prueba\n",
        "preguntas = [\n",
        "    \"¬øQu√© es Python y para qu√© se usa?\",\n",
        "    \"¬øC√≥mo funciona RAG?\",\n",
        "    \"¬øQu√© modelos desarrolla OpenAI?\"\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    resultado = sistema.consultar(pregunta)\n",
        "    print(f\"\\nüí¨ Respuesta:\\n{resultado['respuesta']}\\n\")\n",
        "    print(f\"üìö Fuentes usadas: {resultado['fuentes']}\")\n",
        "    if 'advertencia' in resultado:\n",
        "        print(f\"‚ö†Ô∏è {resultado['advertencia']}\")\n",
        "    print(\"=\" * 60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ESPACIO PARA TU C√ìDIGO\n",
        "class SistemaContextEngineeringAvanzado(SistemaContextEngineering):\n",
        "    \"\"\"\n",
        "    Extensi√≥n del sistema con caracter√≠sticas de producci√≥n\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO: Agrega atributos para m√©tricas, feedback, logs\n",
        "        self.metricas = {\n",
        "            \"total_consultas\": 0,\n",
        "            \"tiempo_promedio\": 0,\n",
        "            \"consultas_exitosas\": 0\n",
        "        }\n",
        "        self.feedback = []\n",
        "        self.logs = []\n",
        "    \n",
        "    def consultar_con_filtros(self, pregunta: str, filtros: Dict = None, top_k: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Consulta con filtros de metadatos\n",
        "        \"\"\"\n",
        "        # TODO: Implementa filtrado por metadatos\n",
        "        # - Filtrar documentos seg√∫n metadatos antes de buscar\n",
        "        # - Aplicar filtros en la b√∫squeda\n",
        "        pass\n",
        "    \n",
        "    def registrar_feedback(self, consulta_id: str, util: bool, comentario: str = \"\"):\n",
        "        \"\"\"\n",
        "        Registra feedback del usuario\n",
        "        \"\"\"\n",
        "        # TODO: Implementa sistema de feedback\n",
        "        pass\n",
        "    \n",
        "    def obtener_metricas(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Retorna m√©tricas del sistema\n",
        "        \"\"\"\n",
        "        # TODO: Calcula y retorna m√©tricas\n",
        "        pass\n",
        "    \n",
        "    def exportar_logs(self, archivo: str = \"logs_consultas.json\"):\n",
        "        \"\"\"\n",
        "        Exporta logs a archivo JSON\n",
        "        \"\"\"\n",
        "        # TODO: Implementa exportaci√≥n de logs\n",
        "        pass\n",
        "\n",
        "# Prueba el sistema avanzado\n",
        "# sistema_avanzado = SistemaContextEngineeringAvanzado()\n",
        "# resultado = sistema_avanzado.consultar(\"¬øQu√© es Python?\")\n",
        "# sistema_avanzado.registrar_feedback(\"consulta_1\", True, \"Muy √∫til\")\n",
        "# print(sistema_avanzado.obtener_metricas())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî• Ejercicio de Desaf√≠o 3.3 (Opcional)\n",
        "\n",
        "Crea un sistema de context engineering de nivel empresarial que incluya:\n",
        "\n",
        "1. **Ingesti√≥n autom√°tica**: Cargar documentos desde m√∫ltiples fuentes (S3, Google Drive, APIs)\n",
        "2. **Pipeline de procesamiento**: Limpieza, normalizaci√≥n, enriquecimiento autom√°tico\n",
        "3. **Versionado de documentos**: Trackear cambios en documentos y re-indexar cuando sea necesario\n",
        "4. **Sistema de permisos**: Control de acceso a diferentes documentos seg√∫n usuario\n",
        "5. **Dashboard de analytics**: Visualizaci√≥n de uso, popularidad de documentos, calidad de respuestas\n",
        "\n",
        "**Desaf√≠o extra**: Implementa un sistema de aprendizaje que mejore autom√°ticamente los prompts bas√°ndose en feedback hist√≥rico.\n",
        "\n",
        "---\n",
        "\n",
        "# üìù Reflexi√≥n y Mejores Pr√°cticas\n",
        "\n",
        "## ‚úÖ ¬øQu√© Aprendimos?\n",
        "\n",
        "### 1. RAG B√°sico\n",
        "- **Embeddings**: Representaciones vectoriales capturan significado sem√°ntico\n",
        "- **B√∫squeda sem√°ntica**: Encontrar informaci√≥n por significado, no solo palabras clave\n",
        "- **Chunking**: Dividir documentos grandes preservando contexto\n",
        "- **Contexto en prompts**: A√±adir informaci√≥n relevante mejora respuestas\n",
        "\n",
        "### 2. RAG Avanzado\n",
        "- **M√∫ltiples fuentes**: Combinar diferentes tipos de datos\n",
        "- **Agregaci√≥n inteligente**: Decidir qu√© fuentes usar seg√∫n la pregunta\n",
        "- **Filtrado**: Usar metadatos para refinar b√∫squedas\n",
        "- **Re-ranking**: Mejorar resultados iniciales\n",
        "\n",
        "### 3. Sistemas Completos\n",
        "- **Cach√©**: Optimizar costos y latencia\n",
        "- **Validaci√≥n**: Verificar calidad de respuestas\n",
        "- **M√©tricas**: Monitorear performance del sistema\n",
        "- **Feedback loops**: Mejorar continuamente\n",
        "\n",
        "## ‚ö†Ô∏è Errores Comunes a Evitar\n",
        "\n",
        "1. **Chunks demasiado peque√±os o grandes**\n",
        "   - ‚ùå Chunks de 50 palabras (pierden contexto)\n",
        "   - ‚úÖ Chunks de 200-500 palabras con overlap\n",
        "\n",
        "2. **Sin validaci√≥n de respuestas**\n",
        "   - ‚ùå Confiar ciegamente en lo que genera el modelo\n",
        "   - ‚úÖ Validar que la respuesta es relevante y precisa\n",
        "\n",
        "3. **Ignorar metadatos**\n",
        "   - ‚ùå Buscar en todos los documentos siempre\n",
        "   - ‚úÖ Usar filtros para reducir b√∫squeda\n",
        "\n",
        "4. **Sin cach√© de embeddings**\n",
        "   - ‚ùå Recalcular embeddings cada vez\n",
        "   - ‚úÖ Cach√©ar embeddings de documentos est√°ticos\n",
        "\n",
        "5. **Prompts no optimizados**\n",
        "   - ‚ùå Prompt gen√©rico para todas las preguntas\n",
        "   - ‚úÖ Adaptar prompt seg√∫n tipo de pregunta y contexto\n",
        "\n",
        "## üöÄ Tips para Producci√≥n\n",
        "\n",
        "1. **Chunking estrat√©gico**: \n",
        "   - Documentos t√©cnicos: 300-500 palabras\n",
        "   - Conversaciones: Por turno completo\n",
        "   - C√≥digo: Por funci√≥n/clase\n",
        "\n",
        "2. **Optimizaci√≥n de embeddings**:\n",
        "   - Usar modelos m√°s peque√±os para documentos\n",
        "   - Modelos m√°s grandes solo para queries cr√≠ticas\n",
        "   - Batch processing para m√∫ltiples documentos\n",
        "\n",
        "3. **B√∫squeda h√≠brida**:\n",
        "   - Combinar b√∫squeda sem√°ntica + keyword\n",
        "   - Re-ranking con m√∫ltiples se√±ales\n",
        "   - Filtrado por metadatos antes de b√∫squeda\n",
        "\n",
        "4. **Monitoreo**:\n",
        "   - Trackear latencia de cada componente\n",
        "   - Monitorear calidad de respuestas (feedback)\n",
        "   - Alertas para degradaci√≥n de performance\n",
        "\n",
        "5. **Escalabilidad**:\n",
        "   - Usar vector stores especializados (Pinecone, Weaviate)\n",
        "   - Implementar sharding para grandes vol√∫menes\n",
        "   - Cach√© distribuido para alta concurrencia\n",
        "\n",
        "6. **Seguridad**:\n",
        "   - Validar inputs de usuario\n",
        "   - Sanitizar documentos antes de indexar\n",
        "   - Control de acceso a documentos sensibles\n",
        "\n",
        "## üìö Recursos Adicionales\n",
        "\n",
        "- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)\n",
        "- [Pinecone Vector Database](https://www.pinecone.io/learn/)\n",
        "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
        "- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Conclusi√≥n del Curso\n",
        "\n",
        "¬°Felicitaciones por completar los 3 m√≥dulos del curso! üéâ\n",
        "\n",
        "Has aprendido:\n",
        "- ‚úÖ Fundamentos de prompt engineering\n",
        "- ‚úÖ T√©cnicas avanzadas para sistemas\n",
        "- ‚úÖ Context engineering y RAG\n",
        "\n",
        "**Pr√≥ximos pasos sugeridos**:\n",
        "1. Implementa un proyecto real usando estas t√©cnicas\n",
        "2. Experimenta con diferentes modelos y configuraciones\n",
        "3. √önete a comunidades de prompt engineering\n",
        "4. Mantente actualizado con las √∫ltimas t√©cnicas\n",
        "\n",
        "¬°√âxito en tus proyectos con IA! üöÄ"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
